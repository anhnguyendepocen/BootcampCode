{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyStan gamma-Poisson MLM example for AAS 227\n",
    "\n",
    "Created 2014-11-04 by Tom Loredo for IAC Winter School\n",
    "\n",
    "Adapted for AAS227 2016-01-06\n",
    "\n",
    "This notebook demonstrates the use of the Stan probabilistic programming language to implement a basic Bayesian multilevel model, the gamma-Poisson model, in a mock astronomical setting. A population of sources is modeled as having fluxes distributed according to a *gamma distribution* (a power-law distribution with an exponential upper cutoff, like a *Schecter function*, but constrained to be normalizable at small fluxes).  Noisy observations are simulated, based on simple Poisson photon counting models for observations of varied exposure (and thus varied precision or signal-to-noise). The goal is to recover the population distribution (i.e., the power-law slope and the cutoff flux) from the observed photon counts.\n",
    "\n",
    "This is but a caricature of what would be required in analyses of real data pertaining to the number counts, $\\log N$&ndash;$\\log S$, or number-size distribution of a population.  Most notably, there is no background, no detection threshold, and thus no accounting for selection effects (which are thus absent in the simulations).  It is meant only as a simple demonstration of the potential of Stan and other probabilistic languages for astronomical data analysis.\n",
    "\n",
    "The notebook was developed using the following tools:\n",
    "* Anaconda Python 2.7.11 ([install Anaconda](http://docs.continuum.io/anaconda/install)), which includes the standard PyData packages (NumPy, SciPy, matplotlib, etc.)\n",
    "* Jupyter 4.0 (included in recent Anaconda installations)\n",
    "* PyStan 2.9.0.0 (install the current PyStan via `pip install pystan`)\n",
    "\n",
    "It also uses two supporting Python modules:\n",
    "* `shrinkage_plot`\n",
    "* `stanfitter`\n",
    "\n",
    "The `shrinkage_plot` module provides a helper function to produce a simple parallel coordinates plot depicting shrinkage of flux estimates. \"Shrinkage\" is a technical term in multivariate statistics, referring to an effect related to what astronomers know as correction for Eddington bias. \n",
    "\n",
    "The `stanfitter` module provides a more \"Pythonic\" wrapper around PyStan's interface to Stan.  It is being maintained separately on GitHub (see [`stanfitter` on GitHub](https://github.com/tloredo/stanfitter)).  The current version is provided in this notebook's GitHub repo for simplicity. It is not well documented (I've been using it in the classroom, teaching its use interactively in lab sessions). Documentation is forthcoming, but this notebook illustrates key capability by example.\n",
    "\n",
    "For information about PyStan and the Stan language, visit:\n",
    "\n",
    "https://pystan.readthedocs.org/en/latest/getting_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Pollute the namespace!\n",
    "from matplotlib.pyplot import *\n",
    "from scipy import *\n",
    "\n",
    "from stanfitter import StanFitter\n",
    "from shrinkage_plot import shrinkage_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get TL's interactive plotting customizations if availalbe.\n",
    "try:\n",
    "    import myplot\n",
    "    from myplot import close_all, csavefig\n",
    "    ion()\n",
    "    # myplot.tex_on()\n",
    "    csavefig.save = False\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stan code for a gamma-Poisson MLM\n",
    "\n",
    "Stan is a sophisticated computing language. When used via PyStan, Stan code may be included in a string (as in the next cell), or in a separate file. This notebook's repo includes a file, \"gamma-poisson.stan\", that shows what a Stan file looks like (it's just the string below, in a file of its own); it is not used by this notebook.\n",
    "\n",
    "Note here that Stan defines the gamma distribution's cutoff, not in terms of a scale parameter (i.e., the flux cutoff), but in terms of the *inverse* scale, traditionally referred to as the $\\beta$ parameter. To handle this, we use Stan's capability to define **transformed parameters**, i.e., functions of the parameters that will actually be used in Stan's calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stan code defining a gamma-Poisson MLM for number counts (log N - log S)\n",
    "# fitting:\n",
    "code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N; \n",
    "    int<lower=0> counts[N];\n",
    "    real  exposures[N]; \n",
    "} \n",
    "\n",
    "parameters {\n",
    "    real<lower=0> alpha; \n",
    "    real<lower=0> beta;\n",
    "    real<lower=0> fluxes[N];\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "    real<lower=0> flux_cut;\n",
    "    flux_cut = 1./beta;\n",
    "}\n",
    "\n",
    "model {\n",
    "    alpha ~ exponential(1.0);\n",
    "    beta ~ gamma(0.1, 0.1);\n",
    "    for (i in 1:N){\n",
    "        fluxes[i] ~ gamma(alpha, beta);\n",
    "        counts[i] ~ poisson(fluxes[i] * exposures[i]);\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem\n",
    "\n",
    "**Choose one of the following two options (stellar or GRB flux dist'ns)**\n",
    "\n",
    "We define some variables here that describe how the data will be simulated and modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup \"true\" model and design parameters for GRB observations:\n",
    "if True:\n",
    "    # Define gamma dist'n parameters alpha & F_cut:\n",
    "    F_cut = 1.  # peak flux of bright BATSE GRB, photons/s/cm^2\n",
    "    alpha = .66  # power law part has exponent alpha-1; requires alpha > 0\n",
    "\n",
    "    # Variables describing the data sample:\n",
    "    n_s = 20\n",
    "    area = 335.  # Single BATSE LAD effective area, cm^2\n",
    "    # Fake projected areas for a triggered detector:\n",
    "    areas = area*stats.uniform(loc=.5, scale=.5).rvs(n_s)\n",
    "    exposures = .064*areas  # use 64 ms peak flux time scale\n",
    "\n",
    "# OR, setup \"true\" model and design parameters for stellar observations:\n",
    "else:\n",
    "    # Define gamma dist'n parameters alpha & F_cut:\n",
    "    Jy_V0 = 3640.  # V=0 energy flux in Jy\n",
    "    phi_V0 = 1.51e7 * 0.16 * Jy_V0  # V=0 photon number flux (s m^2)^{-1}\n",
    "    V_cut = 24.  # V magnitude at rollover\n",
    "    F_cut = phi_V0 * 10.**(-0.4*V_cut)  # flux at rollover\n",
    "    alpha = .4  # power law part has exponent alpha-1; requires alpha > 0\n",
    "\n",
    "    # Variables describing the data sample:\n",
    "    n_s = 25\n",
    "    area = pi*(8.4**2 - 5**2)  # LSST primary area (m^2)\n",
    "    exposures = 10.*area*ones(n_s)  # LSST single-image integration time * area\n",
    "    mid = n_s//2\n",
    "    exposures[mid:] *= 10  # last half use 10x default exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the true flux dist'n as a gamma dist'n.\n",
    "beta = 1./F_cut  # Stan uses the inverse scale\n",
    "ncdistn = stats.gamma(a=alpha, scale=F_cut)\n",
    "\n",
    "# Sample some source fluxes from the flux population dist'n.\n",
    "fluxes = ncdistn.rvs(n_s)\n",
    "\n",
    "\n",
    "# Generate observations of the flux sample.\n",
    "def gen_data():\n",
    "    \"\"\"\n",
    "    Simulate photon count data from the Poisson distribution, gathering\n",
    "    the data and descriptive information in a dict as needed by Stan.\n",
    "    \"\"\"\n",
    "    n_exp = fluxes*exposures  # expected counts for each source\n",
    "    counts = stats.poisson.rvs(n_exp)\n",
    "    return dict(N=n_s, exposures=exposures, counts=counts)\n",
    "\n",
    "data = gen_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke Stan to build the model\n",
    "\n",
    "Invoke stan via the StanFitter class. It takes Stan code either as a string (as here), or via a file.\n",
    "\n",
    "Stan will \"write\" and compile a C++ executable implementing the model, a\n",
    "posterior sampler, Markov chain output analysis functions, and an optimizer.\n",
    "This takes a bit of time. StanFitter will cache the Stan products so\n",
    "subsequent runs of the script need not rebuild the model from scratch.\n",
    "\n",
    "**NOTE:** PyStan's compilation and simulation progress will be reported on stdout. *When using PyStan, make sure you check your Jupyter session's console window for activity and warnings.* If Stan has trouble, you likely won't see any activity in the notebook indicating there was a problem. In particular, if there's a bug in the Stan code, the error message will appear on the console, not in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fitter = StanFitter(code, data)  # use code string in this notebook (above); StanFitter also supports external files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do MCMC posterior sampling via Hamiltonian Monte Carlo\n",
    "\n",
    "A `StanFitter` instance has a `sample()` method that invokes Stan's default HMC \"No-U-Turn Sampler\" (NUTS). It returns a `Fit` object providing access to many results of the sampling run.\n",
    "\n",
    "Remember, *progress is logged to the console window*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run 4 chains of length 2000 (Stan will use 1/2 of each for burn-in).\n",
    "fit = fitter.sample(8000, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick summary\n",
    "\n",
    "Print a quick textual summary of the MCMC results for all parameters and\n",
    "the log posterior density, log_p.  For the vector parameter, `fluxes`, a\n",
    "summary is printed for *every* element, which may not be desired if there\n",
    "are many such parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check convergence and mixing of chains\n",
    "\n",
    "After sampling, the `fit` object has attributes for each parameter and\n",
    "transformed parameter in the model; the attributes provide access to\n",
    "the chains (4 chains in this case), a pooled and thinned collection of samples from all chains, and\n",
    "various summary statistics for each parameter (including MCMC output\n",
    "diagnostics).\n",
    "\n",
    "Verify convergence by looking at the Gelman-Rubin $\\hat R$ statistic for every\n",
    "parameter of interest; it should be within a few % of 1. Also, examine the effective sample size (ESS) as a first indication of how well the chains are mixing. As a rule of thumb, if you aim to report 95% credible regions for parameters, you want the ESS to be at least a few thousand.\n",
    "\n",
    "**Pontification:** Too many Bayesian analyses in the astronomical literature fail to describe adequate MCMC output analysis (most likely because the authors failed to *do* adequate output analysis). For brevity, this notebook presents the *bare minimum* of what you should be doing. If your problem requires MCMC, then it also requires you to really learn about how to do MCMC right. A good starting point is this standard reference:\n",
    "\n",
    "* *Handbook of Markov Chain Monte Carlo* ([authors' web site](http://www.mcmchandbook.net/HandbookTableofContents.html), [publisher's web site](https://www.crcpress.com/Handbook-of-Markov-Chain-Monte-Carlo/Brooks-Gelman-Jones-Meng/9781420079418))\n",
    "\n",
    "Note that some of the key chapters are *freely available* at the authors' web site. Don't \n",
    "use MCMC methods unless you've at least read Charlie Geyer's [\"Introduction to MCMC\"](http://www.mcmchandbook.net/HandbookChapter1.pdf), esp. section 1.11 on \"The Practice of MCMC.\" Not all experts practitioners agree with all of Geyer's advice (particularly in regard to the value of parallel chains), but the vast majority of his recommendations and insights are shared by experts. Some notable excerpts, to motivate you:\n",
    "\n",
    "> There is a great deal of theory about convergence of Markov chains. Unfortunately, none\n",
    "of it can be applied to get useful convergence information for most MCMC applications.\n",
    "Thus most users find themselves in the... situation we call *black box MCMC*....\n",
    "\n",
    "> When you are in the black box situation, you have no idea how long runs need to be to\n",
    "get good mixing (convergence rather than pseudo-convergence)....\n",
    "\n",
    "> Your humble author has a dictum that the least one can do is to make an overnight run.\n",
    "What better way for your computer to spend its time? In many problems that are not too\n",
    "complicated, this is millions or billions of iterations. If you do not make runs like that, you\n",
    "are simply not serious about MCMC. Your humble author has another dictum (only slightly\n",
    "facetious) that one should start a run when the paper is submitted and keep running until the\n",
    "referees’ reports arrive. This cannot delay the paper, and may detect pseudo-convergence.\n",
    "\n",
    "The [PyMC](https://pymc-devs.github.io/pymc/) package provides several tools for MCMC output analysis. Two R packages that statisticians have developed for MCMC output analysis are [`coda`](https://cran.r-project.org/web/packages/coda/index.html) and [`BOA`](http://www.public-health.uiowa.edu/boa/). You may be able to acess them from Python via [`RPy2`](http://rpy.sourceforge.net/) (note that Anaconda now has an [R-Essential channel](https://www.continuum.io/blog/developer/jupyter-and-conda-r), providing support for many popular R packages in conda environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For scalar params, just make a table of Rhat and ESS.\n",
    "scalars = [fit.alpha, fit.beta, fit.flux_cut]\n",
    "print '*** Checks for convergence, mixing ***'\n",
    "print 'Rhat, ESS for scalar params:'\n",
    "for param in scalars:\n",
    "    print '    {0:12s}:  {1:6.3f}  {2:6.0f}'.format(param.name, param.Rhat, param.ess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the vector of latent fluxes, make a plot.\n",
    "flux_Rhats = [fit.fluxes[i].Rhat for i in range(n_s)]\n",
    "figure()\n",
    "subplots_adjust(top=.925, right=.875)  # make room for title, right ESS labels\n",
    "ax_left = subplot(111)  # left axis for Rhat\n",
    "plot(range(n_s), flux_Rhats, 'ob')\n",
    "ylim(0.8, 1.2)\n",
    "axhline(y=1., ls='--', color='k')\n",
    "title('Rhat, ESS for fluxes')\n",
    "xlabel('Source #')\n",
    "ylabel(r'$\\hat R$', color='b')\n",
    "ax_right = twinx()  # right axis for ESS\n",
    "flux_ess = [fit.fluxes[i].ess for i in range(n_s)]\n",
    "plot(range(n_s), flux_ess, 'og')\n",
    "title('Rhat, ESS for fluxes')\n",
    "ylabel('ESS', color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check mixing of chains\n",
    "\n",
    "Check mixing further by examining trace plots of parameters of interest,\n",
    "making sure there are no obvious trends or strong, long-range correlations.\n",
    "Here we look at the scalars alpha, beta (flux_cut is derived from beta so needn't be separately checcked). Another common diagnostic (skipped here) is to examine the autocorrelation functions of traces (in fact, these can be used to compute the ESS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot traces of scalar parameters; distinguish the 4 parallel chains by color.\n",
    "f=figure(figsize=(10,8))\n",
    "ax=f.add_subplot(2,1,1)\n",
    "fit.alpha.trace(axes=ax,alpha=.6)  # without `axes`, this will make its own fig\n",
    "ax=f.add_subplot(2,1,2)\n",
    "fit.beta.trace(axes=ax,alpha=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at traces for some fluxes.\n",
    "f=figure(figsize=(10,8))\n",
    "ax=f.add_subplot(3,1,1)\n",
    "fit.fluxes[0].trace(axes=ax,alpha=.6)\n",
    "ax=f.add_subplot(3,1,2)\n",
    "fit.fluxes[3].trace(axes=ax,alpha=.6)\n",
    "ax=f.add_subplot(3,1,3)\n",
    "fit.fluxes[8].trace(axes=ax,alpha=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the log_p trace plot.\n",
    "fit.log_p.trace()  # creates a new fig by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "\n",
    "Now, **after** the checks, we're ready to make some inferences.\n",
    "\n",
    "Show the joint distribution for (alpha,flux_cut) as a scatterplot, and\n",
    "marginals as histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = figure(figsize=(10,8))\n",
    "subplots_adjust(bottom=.1, left=.1, right=.975, wspace=.24, )\n",
    "\n",
    "# subplot(232)  # joint at mid-top\n",
    "f.add_axes([.25, .6, .5, .36])\n",
    "plot(fit.alpha.thinned, log10(fit.flux_cut.thinned), 'b.', alpha=.1)\n",
    "# crosshair showing true values:\n",
    "xhair = { 'color' : 'r', 'linestyle' : ':' , 'linewidth' : '2'}\n",
    "axvline(alpha, **xhair)\n",
    "axhline(log10(F_cut), **xhair)\n",
    "xlabel(r'$\\alpha$')\n",
    "ylabel(r'$\\log_{10}F_{c}$')\n",
    "\n",
    "subplot(223)  # marginal for alpha bottom-left\n",
    "hist(fit.alpha.thinned, 20, alpha=.4)\n",
    "axvline(alpha, **xhair)\n",
    "xlabel(r'$\\alpha$')\n",
    "ylabel(r'$p(\\alpha|D)$')\n",
    "\n",
    "subplot(224)  # marginal for F_cut bottom-right\n",
    "hist(log10(fit.flux_cut.thinned), 20, alpha=.4)\n",
    "axvline(log10(F_cut), **xhair)\n",
    "xlabel(r'$\\log_{10}F_{c}$')\n",
    "ylabel(r'$p(\\log_{10}F_c|D)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot illustrating shrinkage of point estimates for the fluxes. The shrinkage is pretty minimal for bright fluxes (as one would expect), but significant for those with the noisiest measurements (some noisy cases may have ML estimates that fall out of bounds in the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Max likelihood estimates:\n",
    "F_ml = data['counts']/data['exposures']\n",
    "# Means of marginal posteriors:\n",
    "F_post = array([fit.fluxes[i].mean for i in range(n_s)])\n",
    "#F_vals = linspace(.0001, 1.1*fluxes.max(), 200)  # fluxes for PDF plot\n",
    "u = max(1.1*fluxes.max(), 4*F_cut)\n",
    "F_vals = logspace(-4, log10(u), 200)  # fluxes for PDF plot\n",
    "pdf_vals = ncdistn.pdf(F_vals)  # true number count dist'n over F_vals\n",
    "ax_pdf, ax_pts = shrinkage_plot(F_vals, pdf_vals, fluxes, F_ml, F_post, r'$F$',\n",
    "                 log_x=True, log_y=False)\n",
    "\n",
    "# Get a 'best fit' set of parameters by finding the sample with highest\n",
    "# posterior density; this would be a bad idea in high dimensions (use Stan's\n",
    "# optimizer in such cases).\n",
    "i = fit.log_p.thinned.argmax()\n",
    "a, F = fit.alpha.thinned[i], fit.flux_cut.thinned[i]\n",
    "# Plot the PDF for the best-fit model.\n",
    "best = stats.gamma(a=a, scale=F)\n",
    "ax_pdf.semilogx(F_vals, F_vals*best.pdf(F_vals), 'g--', lw=2, label='Est.')\n",
    "ax_pdf.legend(frameon=False)\n",
    "\n",
    "# Show the PDFs for some posterior samples.\n",
    "for i in 49*linspace(1,10,20,dtype=int):  # every 49th sample\n",
    "    a, F = fit.alpha.thinned[i], fit.flux_cut.thinned[i]\n",
    "    distn = stats.gamma(a=a, scale=F)\n",
    "    ax_pdf.semilogx(F_vals, F_vals*distn.pdf(F_vals), 'k', lw=1, alpha=.5, label=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
